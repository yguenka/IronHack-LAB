{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLBUDdX8SEjl"
   },
   "source": [
    "# Parallelization Lab\n",
    "\n",
    "In this lab, you will be leveraging several concepts you have learned to obtain a list of links from a web page and crawl and index the pages referenced by those links - both sequentially and in parallel. Follow the steps below to complete the lab.\n",
    "\n",
    "### Step 1: Use the requests library to retrieve the content from the URL below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HlQZvSUqSEjn"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://en.wikipedia.org/wiki/Data_science'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHlgfatrSEjs"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "html = response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ig5G0TTDSEjw"
   },
   "source": [
    "### Step 2: Use BeautifulSoup to extract a list of all the unique links on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zahpf2s-SEjx"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VfmrZsypSEj0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"/wiki/Academic_journal\"',\n",
       " '\"/wiki/Academic_publishing\"',\n",
       " '\"/wiki/Academy\"',\n",
       " '\"/wiki/American_Statistical_Association\"',\n",
       " '\"/wiki/Analytics\"',\n",
       " '\"/wiki/Anomaly_detection\"',\n",
       " '\"/wiki/Applied_science\"',\n",
       " '\"/wiki/Artificial_neural_network\"',\n",
       " '\"/wiki/Association_rule_learning\"',\n",
       " '\"/wiki/Autoencoder\"',\n",
       " '\"/wiki/Automated_machine_learning\"',\n",
       " '\"/wiki/BIRCH\"',\n",
       " '\"/wiki/Basic_research\"',\n",
       " '\"/wiki/Bayesian_network\"',\n",
       " '\"/wiki/Big_data\"',\n",
       " '\"/wiki/Bootstrap_aggregating\"',\n",
       " '\"/wiki/CURE_data_clustering_algorithm\"',\n",
       " '\"/wiki/Canonical_correlation\"',\n",
       " '\"/wiki/Cluster_analysis\"',\n",
       " '\"/wiki/Computational_learning_theory\"',\n",
       " '\"/wiki/Computational_science\"',\n",
       " '\"/wiki/Computer_science\"',\n",
       " '\"/wiki/Computing\"',\n",
       " '\"/wiki/Conditional_random_field\"',\n",
       " '\"/wiki/Convolutional_neural_network\"',\n",
       " '\"/wiki/DBSCAN\"',\n",
       " '\"/wiki/DJ_Patil\"',\n",
       " '\"/wiki/Data_analysis\"',\n",
       " '\"/wiki/Data_mining\"',\n",
       " '\"/wiki/Data_science\"',\n",
       " '\"/wiki/Data_set\"',\n",
       " '\"/wiki/Database\"',\n",
       " '\"/wiki/David_Donoho\"',\n",
       " '\"/wiki/Decision_tree_learning\"',\n",
       " '\"/wiki/DeepDream\"',\n",
       " '\"/wiki/Deep_learning\"',\n",
       " '\"/wiki/Digital_object_identifier\"',\n",
       " '\"/wiki/Dimensionality_reduction\"',\n",
       " '\"/wiki/Distributed_computing\"',\n",
       " '\"/wiki/Empirical_research\"',\n",
       " '\"/wiki/Empirical_risk_minimization\"',\n",
       " '\"/wiki/Ensemble_learning\"',\n",
       " '\"/wiki/Factor_analysis\"',\n",
       " '\"/wiki/Feature_engineering\"',\n",
       " '\"/wiki/Feature_learning\"',\n",
       " '\"/wiki/Gated_recurrent_unit\"',\n",
       " '\"/wiki/Generative_adversarial_network\"',\n",
       " '\"/wiki/Glossary_of_artificial_intelligence\"',\n",
       " '\"/wiki/Graduate_school\"',\n",
       " '\"/wiki/Grammar_induction\"',\n",
       " '\"/wiki/Graphical_model\"',\n",
       " '\"/wiki/Harvard_Business_Review\"',\n",
       " '\"/wiki/Health_science\"',\n",
       " '\"/wiki/Hidden_Markov_model\"',\n",
       " '\"/wiki/Hierarchical_clustering\"',\n",
       " '\"/wiki/Independent_component_analysis\"',\n",
       " '\"/wiki/Indian_Statistical_Institute\"',\n",
       " '\"/wiki/Industry\"',\n",
       " '\"/wiki/Information_explosion\"',\n",
       " '\"/wiki/Information_science\"',\n",
       " '\"/wiki/Inter-disciplinary\"',\n",
       " '\"/wiki/Interdisciplinarity\"',\n",
       " '\"/wiki/International_Conference_on_Machine_Learning\"',\n",
       " '\"/wiki/International_Standard_Book_Number\"',\n",
       " '\"/wiki/International_Standard_Serial_Number\"',\n",
       " '\"/wiki/Jeff_Hammerbacher\"',\n",
       " '\"/wiki/Journal_of_Machine_Learning_Research\"',\n",
       " '\"/wiki/K-means_clustering\"',\n",
       " '\"/wiki/K-nearest_neighbors_algorithm\"',\n",
       " '\"/wiki/K-nearest_neighbors_classification\"',\n",
       " '\"/wiki/Knowledge\"',\n",
       " '\"/wiki/Learning_to_rank\"',\n",
       " '\"/wiki/Linear_discriminant_analysis\"',\n",
       " '\"/wiki/Linear_regression\"',\n",
       " '\"/wiki/List_of_datasets_for_machine-learning_research\"',\n",
       " '\"/wiki/Local_outlier_factor\"',\n",
       " '\"/wiki/Logistic_regression\"',\n",
       " '\"/wiki/Long_short-term_memory\"',\n",
       " '\"/wiki/Machine_learning\"',\n",
       " '\"/wiki/Main_Page\"',\n",
       " '\"/wiki/Mathematics\"',\n",
       " '\"/wiki/Mean-shift\"',\n",
       " '\"/wiki/Methodology\"',\n",
       " '\"/wiki/Multilayer_perceptron\"',\n",
       " '\"/wiki/NYU_Stern_Center_for_Business_and_Human_Rights\"',\n",
       " '\"/wiki/Naive_Bayes_classifier\"',\n",
       " '\"/wiki/Nate_Silver\"',\n",
       " '\"/wiki/National_Institutes_of_Health\"',\n",
       " '\"/wiki/New_York_University\"',\n",
       " '\"/wiki/Non-negative_matrix_factorization\"',\n",
       " '\"/wiki/OPTICS_algorithm\"',\n",
       " '\"/wiki/Occam_learning\"',\n",
       " '\"/wiki/Online_machine_learning\"',\n",
       " '\"/wiki/Open_science\"',\n",
       " '\"/wiki/Outline_of_machine_learning\"',\n",
       " '\"/wiki/Paradigm\"',\n",
       " '\"/wiki/Pattern_recognition\"',\n",
       " '\"/wiki/Perceptron\"',\n",
       " '\"/wiki/Peter_Naur\"',\n",
       " '\"/wiki/Prasanta_Chandra_Mahalanobis\"',\n",
       " '\"/wiki/Predictive_modelling\"',\n",
       " '\"/wiki/Principal_component_analysis\"',\n",
       " '\"/wiki/Probably_approximately_correct_learning\"',\n",
       " '\"/wiki/Proper_generalized_decomposition\"',\n",
       " '\"/wiki/PubMed_Central\"',\n",
       " '\"/wiki/PubMed_Identifier\"',\n",
       " '\"/wiki/Q-learning\"',\n",
       " '\"/wiki/Random_forest\"',\n",
       " '\"/wiki/Recurrent_neural_network\"',\n",
       " '\"/wiki/Regression_analysis\"',\n",
       " '\"/wiki/Reinforcement_learning\"',\n",
       " '\"/wiki/Relevance_vector_machine\"',\n",
       " '\"/wiki/Restricted_Boltzmann_machine\"',\n",
       " '\"/wiki/Self-organizing_map\"',\n",
       " '\"/wiki/Semi-supervised_learning\"',\n",
       " '\"/wiki/Social_science\"',\n",
       " '\"/wiki/Statistical_classification\"',\n",
       " '\"/wiki/Statistical_learning_theory\"',\n",
       " '\"/wiki/Statistical_theory\"',\n",
       " '\"/wiki/Statistician\"',\n",
       " '\"/wiki/Statistics\"',\n",
       " '\"/wiki/Structured_prediction\"',\n",
       " '\"/wiki/Supervised_learning\"',\n",
       " '\"/wiki/Support-vector_machine\"',\n",
       " '\"/wiki/T-distributed_stochastic_neighbor_embedding\"',\n",
       " '\"/wiki/Temporal_difference_learning\"',\n",
       " '\"/wiki/The_Data_Incubator\"',\n",
       " '\"/wiki/The_Wall_Street_Journal\"',\n",
       " '\"/wiki/Theory\"',\n",
       " '\"/wiki/Turing_award\"',\n",
       " '\"/wiki/U-Net\"',\n",
       " '\"/wiki/University_of_Essex\"',\n",
       " '\"/wiki/University_of_Michigan\"',\n",
       " '\"/wiki/Unstructured_data\"',\n",
       " '\"/wiki/Unsupervised_learning\"',\n",
       " '\"/wiki/Wayback_Machine\"'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "soup = BeautifulSoup(html)\n",
    "soup = str(soup.find_all('a'))\n",
    "soup = re.findall('\"/wiki/[a-zA-Z_-]{1,50}\"', soup)\n",
    "soup = set(soup)\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7TXSNXCSEj3"
   },
   "source": [
    "### Step 3: Use list comprehensions with conditions to clean the link list.\n",
    "\n",
    "There are two types of links, absolute and relative. Absolute links have the full URL and begin with *http* while relative links begin with a forward slash (/) and point to an internal page within the *wikipedia.org* domain. Clean the respective types of URLs as follows.\n",
    "\n",
    "- Absolute Links: Create a list of these and remove any that contain a percentage sign (%).\n",
    "- Relative Links: Create a list of these, add the domain to the link so that you have the full URL, and remove any that contain a percentage sign (%).\n",
    "- Combine the list of absolute and relative links and ensure there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZqaDQ4gSEj4"
   },
   "outputs": [],
   "source": [
    "domain = 'http://wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHDkpTrISEj7"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liPZ-FYLSEkA"
   },
   "source": [
    "### Step 4: Use the os library to create a folder called *wikipedia* and make that the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5I-1EAcSEkA"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jErdx2cdSEkD"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qyDm2n-rSEkG"
   },
   "source": [
    "### Step 5: Write a function called index_page that accepts a link and does the following.\n",
    "\n",
    "- Tries to request the content of the page referenced by that link.\n",
    "- Slugifies the filename using the `slugify` function from the [python-slugify](https://pypi.org/project/python-slugify/) library and adds a .html file extension.\n",
    "    - If you don't already have the python-slugify library installed, you can pip install it as follows: `$ pip3 install python-slugify`.\n",
    "    - To import the slugify function, you would do the following: `from slugify import slugify`.\n",
    "    - You can then slugify a link as follows `slugify(link)`.\n",
    "- Creates a file in the wikipedia folder using the slugified filename and writes the contents of the page to the file.\n",
    "- If an exception occurs during the process above, just `pass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTzUHCpBSEkH"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'slugify'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-64ea5d6dacdf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mslugify\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mslugify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'slugify'"
     ]
    }
   ],
   "source": [
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mm7-pzw4SEkK"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VVDNEdyVSEkM"
   },
   "source": [
    "### Step 6: Sequentially loop through the list of links, running the index_page function each time.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run. \n",
    "\n",
    "_hint: Use tqdm to keep track of the time._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1akpUc5_SEkN"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWa8qXs3SEkQ"
   },
   "source": [
    "### Step 7: Perform the page indexing in parallel and note the difference in performance.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run.\n",
    "\n",
    "Use both methods, i.e., for one hand use the `multiprocess` module to use the function created in the jupyter notebook and run the download in parallel.\n",
    "\n",
    "And for another hand create a python file containing the function to download the file and use the `multiprocessing` module to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSA-oQV9SEkT"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Sx1hLdbTEp5"
   },
   "source": [
    "**BONUS**: Create a function that counts how many files are there in the wikipedia folder using the `os` module. \n",
    "\n",
    "Delete the files from the folder before you run and perform the above solution asynchronously. \n",
    "\n",
    "Use your function to check how many files are being downloaded."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
