{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLBUDdX8SEjl"
   },
   "source": [
    "# Parallelization Lab\n",
    "\n",
    "In this lab, you will be leveraging several concepts you have learned to obtain a list of links from a web page and crawl and index the pages referenced by those links - both sequentially and in parallel. Follow the steps below to complete the lab.\n",
    "\n",
    "### Step 1: Use the requests library to retrieve the content from the URL below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HlQZvSUqSEjn"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://en.wikipedia.org/wiki/Data_science'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHlgfatrSEjs"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)\n",
    "html = response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ig5G0TTDSEjw"
   },
   "source": [
    "### Step 2: Use BeautifulSoup to extract a list of all the unique links on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zahpf2s-SEjx"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VfmrZsypSEj0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"/wiki/American_Statistical_Association\"',\n",
       " '\"/wiki/Andrew_Gelman\"',\n",
       " '\"/wiki/Anomaly_detection\"',\n",
       " '\"/wiki/Apache_Hadoop\"',\n",
       " '\"/wiki/Artificial_neural_network\"',\n",
       " '\"/wiki/Association_rule_learning\"',\n",
       " '\"/wiki/Autoencoder\"',\n",
       " '\"/wiki/Automated_machine_learning\"',\n",
       " '\"/wiki/BIRCH\"',\n",
       " '\"/wiki/Basic_research\"',\n",
       " '\"/wiki/Bayesian_network\"',\n",
       " '\"/wiki/Ben_Fry\"',\n",
       " '\"/wiki/Big_data\"',\n",
       " '\"/wiki/Bootstrap_aggregating\"',\n",
       " '\"/wiki/CURE_data_clustering_algorithm\"',\n",
       " '\"/wiki/Canonical_correlation\"',\n",
       " '\"/wiki/Cluster_analysis\"',\n",
       " '\"/wiki/Clustering\"',\n",
       " '\"/wiki/Committee_on_Data_for_Science_and_Technology\"',\n",
       " '\"/wiki/Computational_learning_theory\"',\n",
       " '\"/wiki/Computational_science\"',\n",
       " '\"/wiki/Computer_science\"',\n",
       " '\"/wiki/Conditional_random_field\"',\n",
       " '\"/wiki/Convolutional_neural_network\"',\n",
       " '\"/wiki/DBSCAN\"',\n",
       " '\"/wiki/DJ_Patil\"',\n",
       " '\"/wiki/Data_analysis\"',\n",
       " '\"/wiki/Data_mining\"',\n",
       " '\"/wiki/Data_science\"',\n",
       " '\"/wiki/Database\"',\n",
       " '\"/wiki/David_Donoho\"',\n",
       " '\"/wiki/Decision_tree_learning\"',\n",
       " '\"/wiki/DeepDream\"',\n",
       " '\"/wiki/Deep_learning\"',\n",
       " '\"/wiki/Digital_object_identifier\"',\n",
       " '\"/wiki/Dimensionality_reduction\"',\n",
       " '\"/wiki/Distributed_computing\"',\n",
       " '\"/wiki/Empirical_research\"',\n",
       " '\"/wiki/Empirical_risk_minimization\"',\n",
       " '\"/wiki/Ensemble_learning\"',\n",
       " '\"/wiki/Factor_analysis\"',\n",
       " '\"/wiki/Feature_engineering\"',\n",
       " '\"/wiki/Feature_learning\"',\n",
       " '\"/wiki/Gated_recurrent_unit\"',\n",
       " '\"/wiki/Generative_adversarial_network\"',\n",
       " '\"/wiki/Glossary_of_artificial_intelligence\"',\n",
       " '\"/wiki/Grammar_induction\"',\n",
       " '\"/wiki/Graphical_model\"',\n",
       " '\"/wiki/Hidden_Markov_model\"',\n",
       " '\"/wiki/Hierarchical_clustering\"',\n",
       " '\"/wiki/Independent_component_analysis\"',\n",
       " '\"/wiki/Information_explosion\"',\n",
       " '\"/wiki/Information_science\"',\n",
       " '\"/wiki/Information_visualization\"',\n",
       " '\"/wiki/Inter-disciplinary\"',\n",
       " '\"/wiki/International_Conference_on_Machine_Learning\"',\n",
       " '\"/wiki/International_Standard_Book_Number\"',\n",
       " '\"/wiki/International_Standard_Serial_Number\"',\n",
       " '\"/wiki/Jeff_Hammerbacher\"',\n",
       " '\"/wiki/John_Tukey\"',\n",
       " '\"/wiki/Journal_of_Machine_Learning_Research\"',\n",
       " '\"/wiki/Jupyter_Notebook\"',\n",
       " '\"/wiki/K-means_clustering\"',\n",
       " '\"/wiki/K-nearest_neighbors_algorithm\"',\n",
       " '\"/wiki/K-nearest_neighbors_classification\"',\n",
       " '\"/wiki/Knowledge\"',\n",
       " '\"/wiki/Learning_to_rank\"',\n",
       " '\"/wiki/Linear_discriminant_analysis\"',\n",
       " '\"/wiki/Linear_regression\"',\n",
       " '\"/wiki/List_of_datasets_for_machine-learning_research\"',\n",
       " '\"/wiki/Local_outlier_factor\"',\n",
       " '\"/wiki/Logistic_regression\"',\n",
       " '\"/wiki/Long_short-term_memory\"',\n",
       " '\"/wiki/Machine_learning\"',\n",
       " '\"/wiki/Main_Page\"',\n",
       " '\"/wiki/Mathematics\"',\n",
       " '\"/wiki/Mean-shift\"',\n",
       " '\"/wiki/Multilayer_perceptron\"',\n",
       " '\"/wiki/Naive_Bayes_classifier\"',\n",
       " '\"/wiki/Nate_Silver\"',\n",
       " '\"/wiki/Nathan_Yau\"',\n",
       " '\"/wiki/National_Science_Board\"',\n",
       " '\"/wiki/Non-negative_matrix_factorization\"',\n",
       " '\"/wiki/OCLC\"',\n",
       " '\"/wiki/OPTICS_algorithm\"',\n",
       " '\"/wiki/Occam_learning\"',\n",
       " '\"/wiki/Online_machine_learning\"',\n",
       " '\"/wiki/Outline_of_machine_learning\"',\n",
       " '\"/wiki/Perceptron\"',\n",
       " '\"/wiki/Peter_Naur\"',\n",
       " '\"/wiki/Principal_component_analysis\"',\n",
       " '\"/wiki/Probably_approximately_correct_learning\"',\n",
       " '\"/wiki/Proper_generalized_decomposition\"',\n",
       " '\"/wiki/PubMed_Identifier\"',\n",
       " '\"/wiki/Pytorch\"',\n",
       " '\"/wiki/Q-learning\"',\n",
       " '\"/wiki/Random_forest\"',\n",
       " '\"/wiki/Recurrent_neural_network\"',\n",
       " '\"/wiki/Regression_analysis\"',\n",
       " '\"/wiki/Reinforcement_learning\"',\n",
       " '\"/wiki/Relevance_vector_machine\"',\n",
       " '\"/wiki/Restricted_Boltzmann_machine\"',\n",
       " '\"/wiki/Self-organizing_map\"',\n",
       " '\"/wiki/Semi-supervised_learning\"',\n",
       " '\"/wiki/Statistical_classification\"',\n",
       " '\"/wiki/Statistical_learning_theory\"',\n",
       " '\"/wiki/Statistics\"',\n",
       " '\"/wiki/Structured_prediction\"',\n",
       " '\"/wiki/Supervised_learning\"',\n",
       " '\"/wiki/Support-vector_machine\"',\n",
       " '\"/wiki/T-distributed_stochastic_neighbor_embedding\"',\n",
       " '\"/wiki/Tableau_Software\"',\n",
       " '\"/wiki/Temporal_difference_learning\"',\n",
       " '\"/wiki/TensorFlow\"',\n",
       " '\"/wiki/Turing_award\"',\n",
       " '\"/wiki/U-Net\"',\n",
       " '\"/wiki/Unstructured_data\"',\n",
       " '\"/wiki/Unsupervised_learning\"',\n",
       " '\"/wiki/Vasant_Dhar\"'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "soup = BeautifulSoup(html)\n",
    "soup = str(soup.find_all('a'))\n",
    "soup = re.findall('\"/wiki/[a-zA-Z_-]{1,50}\"', soup)\n",
    "soup = set(soup)\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7TXSNXCSEj3"
   },
   "source": [
    "### Step 3: Use list comprehensions with conditions to clean the link list.\n",
    "\n",
    "There are two types of links, absolute and relative. Absolute links have the full URL and begin with *http* while relative links begin with a forward slash (/) and point to an internal page within the *wikipedia.org* domain. Clean the respective types of URLs as follows.\n",
    "\n",
    "- Absolute Links: Create a list of these and remove any that contain a percentage sign (%).\n",
    "- Relative Links: Create a list of these, add the domain to the link so that you have the full URL, and remove any that contain a percentage sign (%).\n",
    "- Combine the list of absolute and relative links and ensure there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZqaDQ4gSEj4"
   },
   "outputs": [],
   "source": [
    "domain = 'http://wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHDkpTrISEj7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://wikipedia.org/wiki/Jeff_Hammerbacher',\n",
       " 'http://wikipedia.org/wiki/Temporal_difference_learning',\n",
       " 'http://wikipedia.org/wiki/Ensemble_learning',\n",
       " 'http://wikipedia.org/wiki/Occam_learning',\n",
       " 'http://wikipedia.org/wiki/Autoencoder',\n",
       " 'http://wikipedia.org/wiki/Data_science',\n",
       " 'http://wikipedia.org/wiki/Nathan_Yau',\n",
       " 'http://wikipedia.org/wiki/DBSCAN',\n",
       " 'http://wikipedia.org/wiki/Structured_prediction',\n",
       " 'http://wikipedia.org/wiki/Distributed_computing',\n",
       " 'http://wikipedia.org/wiki/Nate_Silver',\n",
       " 'http://wikipedia.org/wiki/Tableau_Software',\n",
       " 'http://wikipedia.org/wiki/Computational_science',\n",
       " 'http://wikipedia.org/wiki/Multilayer_perceptron',\n",
       " 'http://wikipedia.org/wiki/Association_rule_learning',\n",
       " 'http://wikipedia.org/wiki/Generative_adversarial_network',\n",
       " 'http://wikipedia.org/wiki/Bootstrap_aggregating',\n",
       " 'http://wikipedia.org/wiki/Unsupervised_learning',\n",
       " 'http://wikipedia.org/wiki/Information_visualization',\n",
       " 'http://wikipedia.org/wiki/Unstructured_data',\n",
       " 'http://wikipedia.org/wiki/American_Statistical_Association',\n",
       " 'http://wikipedia.org/wiki/David_Donoho',\n",
       " 'http://wikipedia.org/wiki/Bayesian_network',\n",
       " 'http://wikipedia.org/wiki/TensorFlow',\n",
       " 'http://wikipedia.org/wiki/Mathematics',\n",
       " 'http://wikipedia.org/wiki/K-nearest_neighbors_algorithm',\n",
       " 'http://wikipedia.org/wiki/Clustering',\n",
       " 'http://wikipedia.org/wiki/Committee_on_Data_for_Science_and_Technology',\n",
       " 'http://wikipedia.org/wiki/Outline_of_machine_learning',\n",
       " 'http://wikipedia.org/wiki/Computer_science',\n",
       " 'http://wikipedia.org/wiki/Database',\n",
       " 'http://wikipedia.org/wiki/Statistical_learning_theory',\n",
       " 'http://wikipedia.org/wiki/Feature_learning',\n",
       " 'http://wikipedia.org/wiki/Andrew_Gelman',\n",
       " 'http://wikipedia.org/wiki/Empirical_risk_minimization',\n",
       " 'http://wikipedia.org/wiki/Perceptron',\n",
       " 'http://wikipedia.org/wiki/Dimensionality_reduction',\n",
       " 'http://wikipedia.org/wiki/Statistics',\n",
       " 'http://wikipedia.org/wiki/Knowledge',\n",
       " 'http://wikipedia.org/wiki/Random_forest',\n",
       " 'http://wikipedia.org/wiki/CURE_data_clustering_algorithm',\n",
       " 'http://wikipedia.org/wiki/Gated_recurrent_unit',\n",
       " 'http://wikipedia.org/wiki/Restricted_Boltzmann_machine',\n",
       " 'http://wikipedia.org/wiki/OPTICS_algorithm',\n",
       " 'http://wikipedia.org/wiki/Reinforcement_learning',\n",
       " 'http://wikipedia.org/wiki/DJ_Patil',\n",
       " 'http://wikipedia.org/wiki/Feature_engineering',\n",
       " 'http://wikipedia.org/wiki/DeepDream',\n",
       " 'http://wikipedia.org/wiki/Naive_Bayes_classifier',\n",
       " 'http://wikipedia.org/wiki/Pytorch',\n",
       " 'http://wikipedia.org/wiki/Mean-shift',\n",
       " 'http://wikipedia.org/wiki/International_Conference_on_Machine_Learning',\n",
       " 'http://wikipedia.org/wiki/National_Science_Board',\n",
       " 'http://wikipedia.org/wiki/Main_Page',\n",
       " 'http://wikipedia.org/wiki/Conditional_random_field',\n",
       " 'http://wikipedia.org/wiki/Information_explosion',\n",
       " 'http://wikipedia.org/wiki/John_Tukey',\n",
       " 'http://wikipedia.org/wiki/Turing_award',\n",
       " 'http://wikipedia.org/wiki/Information_science',\n",
       " 'http://wikipedia.org/wiki/Graphical_model',\n",
       " 'http://wikipedia.org/wiki/Big_data',\n",
       " 'http://wikipedia.org/wiki/Vasant_Dhar',\n",
       " 'http://wikipedia.org/wiki/Logistic_regression',\n",
       " 'http://wikipedia.org/wiki/Convolutional_neural_network',\n",
       " 'http://wikipedia.org/wiki/Data_analysis',\n",
       " 'http://wikipedia.org/wiki/Empirical_research',\n",
       " 'http://wikipedia.org/wiki/K-nearest_neighbors_classification',\n",
       " 'http://wikipedia.org/wiki/Q-learning',\n",
       " 'http://wikipedia.org/wiki/Basic_research',\n",
       " 'http://wikipedia.org/wiki/Statistical_classification',\n",
       " 'http://wikipedia.org/wiki/Grammar_induction',\n",
       " 'http://wikipedia.org/wiki/OCLC',\n",
       " 'http://wikipedia.org/wiki/Jupyter_Notebook',\n",
       " 'http://wikipedia.org/wiki/International_Standard_Serial_Number',\n",
       " 'http://wikipedia.org/wiki/Automated_machine_learning',\n",
       " 'http://wikipedia.org/wiki/Artificial_neural_network',\n",
       " 'http://wikipedia.org/wiki/Recurrent_neural_network',\n",
       " 'http://wikipedia.org/wiki/Hierarchical_clustering',\n",
       " 'http://wikipedia.org/wiki/Non-negative_matrix_factorization',\n",
       " 'http://wikipedia.org/wiki/Digital_object_identifier',\n",
       " 'http://wikipedia.org/wiki/Relevance_vector_machine',\n",
       " 'http://wikipedia.org/wiki/BIRCH',\n",
       " 'http://wikipedia.org/wiki/Peter_Naur',\n",
       " 'http://wikipedia.org/wiki/Online_machine_learning',\n",
       " 'http://wikipedia.org/wiki/International_Standard_Book_Number',\n",
       " 'http://wikipedia.org/wiki/Glossary_of_artificial_intelligence',\n",
       " 'http://wikipedia.org/wiki/Supervised_learning',\n",
       " 'http://wikipedia.org/wiki/Data_mining',\n",
       " 'http://wikipedia.org/wiki/U-Net',\n",
       " 'http://wikipedia.org/wiki/Self-organizing_map',\n",
       " 'http://wikipedia.org/wiki/Local_outlier_factor',\n",
       " 'http://wikipedia.org/wiki/Ben_Fry',\n",
       " 'http://wikipedia.org/wiki/Probably_approximately_correct_learning',\n",
       " 'http://wikipedia.org/wiki/Principal_component_analysis',\n",
       " 'http://wikipedia.org/wiki/Semi-supervised_learning',\n",
       " 'http://wikipedia.org/wiki/Computational_learning_theory',\n",
       " 'http://wikipedia.org/wiki/Journal_of_Machine_Learning_Research',\n",
       " 'http://wikipedia.org/wiki/Independent_component_analysis',\n",
       " 'http://wikipedia.org/wiki/Decision_tree_learning',\n",
       " 'http://wikipedia.org/wiki/Machine_learning',\n",
       " 'http://wikipedia.org/wiki/Linear_discriminant_analysis',\n",
       " 'http://wikipedia.org/wiki/Canonical_correlation',\n",
       " 'http://wikipedia.org/wiki/Apache_Hadoop',\n",
       " 'http://wikipedia.org/wiki/PubMed_Identifier',\n",
       " 'http://wikipedia.org/wiki/List_of_datasets_for_machine-learning_research',\n",
       " 'http://wikipedia.org/wiki/Deep_learning',\n",
       " 'http://wikipedia.org/wiki/Linear_regression',\n",
       " 'http://wikipedia.org/wiki/Regression_analysis',\n",
       " 'http://wikipedia.org/wiki/Inter-disciplinary',\n",
       " 'http://wikipedia.org/wiki/Cluster_analysis',\n",
       " 'http://wikipedia.org/wiki/Hidden_Markov_model',\n",
       " 'http://wikipedia.org/wiki/Long_short-term_memory',\n",
       " 'http://wikipedia.org/wiki/Factor_analysis',\n",
       " 'http://wikipedia.org/wiki/Anomaly_detection',\n",
       " 'http://wikipedia.org/wiki/Proper_generalized_decomposition',\n",
       " 'http://wikipedia.org/wiki/K-means_clustering',\n",
       " 'http://wikipedia.org/wiki/Support-vector_machine',\n",
       " 'http://wikipedia.org/wiki/Learning_to_rank',\n",
       " 'http://wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "link = [domain + item.replace('\"', '') for item in soup]\n",
    "link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liPZ-FYLSEkA"
   },
   "source": [
    "### Step 4: Use the os library to create a folder called *wikipedia* and make that the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5I-1EAcSEkA"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jErdx2cdSEkD"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "#os.mkdir('wikipedia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qyDm2n-rSEkG"
   },
   "source": [
    "### Step 5: Write a function called index_page that accepts a link and does the following.\n",
    "\n",
    "- Tries to request the content of the page referenced by that link.\n",
    "- Slugifies the filename using the `slugify` function from the [python-slugify](https://pypi.org/project/python-slugify/) library and adds a .html file extension.\n",
    "    - If you don't already have the python-slugify library installed, you can pip install it as follows: `$ pip3 install python-slugify`.\n",
    "    - To import the slugify function, you would do the following: `from slugify import slugify`.\n",
    "    - You can then slugify a link as follows `slugify(link)`.\n",
    "- Creates a file in the wikipedia folder using the slugified filename and writes the contents of the page to the file.\n",
    "- If an exception occurs during the process above, just `pass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install python-slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTzUHCpBSEkH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http-wikipedia-org-wiki-jeff-hammerbacher'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slugify(link[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "for item in link:\n",
    "    try:\n",
    "        arquivo = open('./wikipedia/' + slugify(item) + '.html', 'wb')\n",
    "        arquivo.write(requests.get(item).content)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VVDNEdyVSEkM"
   },
   "source": [
    "### Step 6: Sequentially loop through the list of links, running the index_page function each time.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run. \n",
    "\n",
    "_hint: Use tqdm to keep track of the time._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1akpUc5_SEkN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in link:\n",
    "    try:\n",
    "        arquivo = open('./wikipedia/' + slugify(item) + '.html', 'wb')\n",
    "        arquivo.write(requests.get(item).content)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWa8qXs3SEkQ"
   },
   "source": [
    "### Step 7: Perform the page indexing in parallel and note the difference in performance.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run.\n",
    "\n",
    "Use both methods, i.e., for one hand use the `multiprocess` module to use the function created in the jupyter notebook and run the download in parallel.\n",
    "\n",
    "And for another hand create a python file containing the function to download the file and use the `multiprocessing` module to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multiprocess\n",
      "  Downloading https://files.pythonhosted.org/packages/58/17/5151b6ac2ac9b6276d46c33369ff814b0901872b2a0871771252f02e9192/multiprocess-0.70.9.tar.gz (1.6MB)\n",
      "Collecting dill>=0.3.1 (from multiprocess)\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
      "Building wheels for collected packages: multiprocess, dill\n",
      "  Building wheel for multiprocess (setup.py): started\n",
      "  Building wheel for multiprocess (setup.py): finished with status 'done'\n",
      "  Created wheel for multiprocess: filename=multiprocess-0.70.9-cp37-none-any.whl size=108035 sha256=baf3b40293d7be7b137c2d72d0d004421439b82075e93b2e9b25731456e90ffd\n",
      "  Stored in directory: C:\\Users\\yukar\\AppData\\Local\\pip\\Cache\\wheels\\96\\20\\ac\\9f1d164f7d81787cd6f4401b1d05212807d021fbbbcc301b82\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-cp37-none-any.whl size=78598 sha256=f59f077a91abfd0618ffbfef43263d4d70d62247ef93bcd2d0d27dd983eb6d9a\n",
      "  Stored in directory: C:\\Users\\yukar\\AppData\\Local\\pip\\Cache\\wheels\\59\\b1\\91\\f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
      "Successfully built multiprocess dill\n",
      "Installing collected packages: dill, multiprocess\n",
      "Successfully installed dill-0.3.1.1 multiprocess-0.70.9\n"
     ]
    }
   ],
   "source": [
    "#!pip install multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSA-oQV9SEkT"
   },
   "outputs": [],
   "source": [
    "def Links(item):\n",
    "    try:\n",
    "        arquivo = open('./wikipedia/' + slugify(item) + '.html', 'wb')\n",
    "        arquivo.write(requests.get(item).content)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 478 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = multiprocess.Pool()\n",
    "result = pool.map(Links, link)\n",
    "pool.terminate()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Sx1hLdbTEp5"
   },
   "source": [
    "**BONUS**: Create a function that counts how many files are there in the wikipedia folder using the `os` module. \n",
    "\n",
    "Delete the files from the folder before you run and perform the above solution asynchronously. \n",
    "\n",
    "Use your function to check how many files are being downloaded."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
